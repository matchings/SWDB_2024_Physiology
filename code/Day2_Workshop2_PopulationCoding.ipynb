{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11244721",
   "metadata": {},
   "source": [
    "![Image](../resources/cropped-SummerWorkshop_Header.png)\n",
    "\n",
    "<h1 align=\"center\">Population Coding</h1> \n",
    "<h2 align=\"center\"> Day 2, Afternoon Session</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f95c14",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "In the first workshop of today, we examined how sensory variables can be encoded in individual neurons' activity. We now turn our attention to the coordinated activity of groups of neurons: population codes!\n",
    "    \n",
    "### How do populations of neurons encode information about sensory stimuli? \n",
    "### How are these population codes modulated by context or behavioral state? \n",
    "### What other types of thing are encoded in population activity?\n",
    "    \n",
    "### We'll address these questions using the Visual Behavior Neuropixels data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c3f11",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "### Extracellular Electrophysiology Data\n",
    "\n",
    "The data from in vivo extracellular electrophysiology experiments are organized into *sessions*, where each session is a distinct continuous recording period. During a session we collect:\n",
    "\n",
    "- spike times and characteristics (such as mean waveforms) from up to 6 neuropixels probes\n",
    "- local field potentials\n",
    "- behavioral data, such as running speed and eye position and lick times\n",
    "- visual stimuli which were presented during the session\n",
    "- cell-type specific optogenetic stimuli that were applied during the session\n",
    "\n",
    "The AllenSDK contains code for accessing across-session (project-level) metadata as well as code for accessing detailed within-session data. The standard workflow is to use project-level tools, such as `EcephysProjectCache` to identify and access sessions of interest, then delve into those sessions' data using `EcephysSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache.\\\n",
    "    behavior_neuropixels_project_cache \\\n",
    "    import VisualBehaviorNeuropixelsProjectCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "if 'Darwin' in platstring:\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2024/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on CodeOcean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2024/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = VisualBehaviorNeuropixelsProjectCache.from_local_cache(cache_dir=data_root, use_static_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6aa9",
   "metadata": {},
   "source": [
    "Grab data from a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cache.get_ecephys_session(\n",
    "           ecephys_session_id=1065437523)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28561c",
   "metadata": {},
   "source": [
    "The stimulus presentations table is a record of every stimulus we presented to the mouse over the course of this experiment. Let's take a look at this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations = session.stimulus_presentations\n",
    "stimulus_presentations.head(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0335b30",
   "metadata": {},
   "source": [
    "It contains a great deal of information about the stimulus trials! Let's look at all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eae999",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fce4f",
   "metadata": {},
   "source": [
    "The experiment is divided into stimulus blocks. During each block a different set of stimuli are presented. A stimulus block can be active or passive. In active blocks, the mouse performs the change detection task introduced earlier. In passive blocks, there is no task.\n",
    "\n",
    "The different stimuli are indexed by the 'stimulus_block' column. Let's group stimulus presentations dataframe by stimulus block and see what stimulus was shown for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations = session.stimulus_presentations\n",
    "stimulus_presentations.groupby('stimulus_block')[['stimulus_block', \n",
    "                                                'stimulus_name', \n",
    "                                                'active', \n",
    "                                                'duration', \n",
    "                                                'start_time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc20e0",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "What are the types of stimulus block that were presented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588d60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2ba6ec2",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "What are the names of the stimuli shown in the 'Natural_Images_Lum_Matched_set_ophys_G_2019' blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations = session.stimulus_presentations\n",
    "stimulus_presentations = stimulus_presentations[stimulus_presentations.stimulus_name == 'Natural_Images_Lum_Matched_set_ophys_G_2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea177525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7200e0",
   "metadata": {},
   "source": [
    "Now let's get unit and channel data, sort the units by depth and filter for \"good\" units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get unit and channel data, sort the units by depth and filter for \"good\" units\n",
    "units = session.get_units() # contains information about spike waveforms, isolation quality\n",
    "channels = session.get_channels() # contains information about anatomical location\n",
    "\n",
    "unit_channels = units.merge(channels, left_on='peak_channel_id', right_index=True) # associate anatomical information with each unit\n",
    "\n",
    "#first let's sort our units by depth and filter\n",
    "unit_channels = unit_channels.sort_values('probe_vertical_position', ascending=False)\n",
    "\n",
    "#now we'll filter them\n",
    "good_unit_filter = ((unit_channels['snr']>1)&\n",
    "                    (unit_channels['isi_violations']<1)&\n",
    "                    (unit_channels['firing_rate']>0.1))\n",
    "\n",
    "good_units = unit_channels.loc[good_unit_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd98a46",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Which brain structures were recorded from in this session? How many units are present in each structure? (Hint: try the \"value_counts\" function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57950288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeae8c77",
   "metadata": {},
   "source": [
    "### For now, let's look at the population activity in primary visual cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_of_interest = 'VISp'\n",
    "area_units = good_units[good_units['structure_acronym'] == area_of_interest]\n",
    "num_units = len(area_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1add6",
   "metadata": {},
   "source": [
    "### Let's start by looking at the neural activity! Does it reflect the image presentation?\n",
    "\n",
    "### The session.spike_times object contains all spike times, in seconds, indexed by the unit ID. Let's take a look at this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc06381",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times = session.spike_times\n",
    "spike_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b3c71",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "\n",
    "Get the array of spike times for unit 1068230173. How many times does this unit spike in the first minute of the experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_spike_times = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af21712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90421fed",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "\n",
    "Plot a population spike raster spanning 1 second before to 1 second after a stimulus presentation. (Complete the two lines in the for loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot a single-trial raster, population PSTH, and representation matrix\n",
    "pre_time = 1\n",
    "post_time = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "presentation_idx = 1 # which trial to center the raster on\n",
    "start_time = stimulus_presentations['start_time'][presentation_idx] # get spike times starting one second before this\n",
    "end_time = stimulus_presentations['end_time'][presentation_idx] # \n",
    "\n",
    "unit_num = 0\n",
    "for iu, unit in area_units.iterrows():\n",
    "    unit_spike_times = spike_times[iu] # a numpy array\n",
    "    \n",
    "    unit_spike_times = \n",
    "    unit_num_spikes = \n",
    "    \n",
    "    ax.plot(unit_spike_times - start_time, unit_num*np.ones(unit_num_spikes,), 'k|', markersize=5)\n",
    "    unit_num += 1\n",
    "\n",
    "ax.set_title('Single-trial raster')\n",
    "ax.set_xlabel('Time relative to stimulus presentation (s)')\n",
    "ax.set_ylabel('Unit')\n",
    "ax.set_ylim((0, num_units+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a4acf",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "Now let's compare to a change trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62007f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_idx = np.where(stimulus_presentations['is_change'].values)[0]\n",
    "presentation_idx = change_idx[0]\n",
    "\n",
    "start_time = stimulus_presentations['start_time'][presentation_idx]\n",
    "end_time = stimulus_presentations['end_time'][presentation_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465c256",
   "metadata": {},
   "source": [
    "### Now let's take a look at the trial-averaged responses to see how a neuron encodes the stimulus in its time-dependent firing rate (its peri-stimulus time histogram, or PSTH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convenience function to compute the PSTH\n",
    "def makePSTH(spikes, startTimes, windowDur, binSize=0.001):\n",
    "    bins = np.arange(0,windowDur+binSize,binSize)\n",
    "    counts = np.zeros(bins.size-1)\n",
    "    for i,start in enumerate(startTimes):\n",
    "        startInd = np.searchsorted(spikes, start)\n",
    "        endInd = np.searchsorted(spikes, start+windowDur)\n",
    "        counts = counts + np.histogram(spikes[startInd:endInd]-start, bins)[0]\n",
    "    \n",
    "    counts = counts/startTimes.size\n",
    "    return counts/binSize, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4818c39",
   "metadata": {},
   "source": [
    "Let's start by plotting the response of unit 0 to one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = stimulus_presentations['image_name'].unique()\n",
    "stimulus = stimuli[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "presentations = stimulus_presentations[stimulus_presentations['image_name'] == stimulus]\n",
    "num_presentations = len(presentations)\n",
    "\n",
    "start_times = presentations['start_time'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = area_units.index\n",
    "iu = unit_ids[5]\n",
    "unit_spike_times = spike_times[iu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_before_im = 1\n",
    "duration = 2\n",
    "\n",
    "unit_response, bins = makePSTH(unit_spike_times, \n",
    "                                  start_times - time_before_im, \n",
    "                                  duration, binSize=0.01)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(bins[:-1] - time_before_im, unit_response)\n",
    "ax.set_xlabel('Time from change (s)')\n",
    "ax.set_ylabel('Firing rate (Hz)')\n",
    "ax.set_title('Peri-stimulus time histogram for {}'.format(stimulus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa75e5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Plot the PSTHs for every unit to that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a311e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75fc0d5e",
   "metadata": {},
   "source": [
    "We can see the trial structure of the task reflected in the PSTH. Some units have very strong transient responss to the image presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8389f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Plot the PSTHs for every unit to another image. Do the same neurons have the strongest responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d1d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091dd7ef",
   "metadata": {},
   "source": [
    "## Training a classifier on population spiking data\n",
    "\n",
    "In order to determine how well we can decode the stimulus direction from population activity, we will train a **classifier** on our matrix of firing rates. Whereas regression models try to predict continuous values from the input features, classification models try to predict *labels* (also known as classes) from the input features.\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "Let's start with a linear Support Vector Machine (SVM) classifier, which will try to draw linear boundaries between orientation conditions (the labels) in our 94-dimensional firing rate space.\n",
    "\n",
    "This cartoon shows how we would expect an SVM to behave on a much simpler dataset, which has two dimensions and three conditions:\n",
    "\n",
    "![SVM illustration](../resources/svm-classifier.png)\n",
    "\n",
    "SVM computes decision boundaries in feature space that can be used to classify different conditions. If a new data point appears, the SVM classifier will label it based on where it falls with respect to these boundaries.\n",
    "\n",
    "To train an SVM, we need to import the following methods from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f21c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2c41c",
   "metadata": {},
   "source": [
    "### First, we need to create a response matrix and vector of stimulus labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3d4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stimulus_presentations = session.stimulus_presentations\n",
    "stimulus_presentations = stimulus_presentations[stimulus_presentations.stimulus_name == 'Natural_Images_Lum_Matched_set_ophys_G_2019']\n",
    "stimulus_presentations = stimulus_presentations[stimulus_presentations.active]\n",
    "\n",
    "num_presentations = len(stimulus_presentations)\n",
    "stimulus_presentations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_response_array(spike_times, stimulus_presentations, units, window=.05):\n",
    "\n",
    "    '''\n",
    "    Create an array of spike counts x stimulus presentations, and a corresponding list of stimulus label\n",
    "    spike_times: spike times \n",
    "    stimulus_presentation: stimulus presentation table\n",
    "    units: units table containing only the units to get the responses of\n",
    "    '''\n",
    "\n",
    "    # sort spike times chronologically; necessary for the binary search later\n",
    "    sorted_spikes = dict()\n",
    "    for iu in units.index:\n",
    "        # mergesort/timsort since most spike_times are already sorted\n",
    "        sorted_spikes[iu] = np.sort(spike_times[iu], kind='mergesort')\n",
    "\n",
    "    # create our own copy of stimulus presentations and sort by presentation start time chronologically\n",
    "    # sortation of stimulus_presentations isn't necessary, but it speeds up the vectorized `searchsorted(...)`\n",
    "    stimulus_presentations = stimulus_presentations.sort_values(by='start_time', kind='mergesort', inplace=False)\n",
    "\n",
    "    # Calculate the duration of stimulus presentations, and drop NaN durations\n",
    "    stimulus_presentations['duration'] = stimulus_presentations['end_time'] - stimulus_presentations['start_time']\n",
    "    stimulus_presentations.dropna(subset='duration', inplace=True)\n",
    "    \n",
    "    # Warn if window size is too big\n",
    "    if np.any(window > stimulus_presentations['duration']):\n",
    "        print('Warning: window size longer than stimulus presentation')\n",
    "\n",
    "    responses_by_unit = list()\n",
    "    for iu in units.index:\n",
    "        unit_spike_times = sorted_spikes[iu]\n",
    "\n",
    "        # Determine the first and last spike time for each stimulus presentation\n",
    "        start_is = np.searchsorted(unit_spike_times, stimulus_presentations['start_time'])\n",
    "        end_is = np.searchsorted(unit_spike_times, stimulus_presentations['start_time']+window)\n",
    "\n",
    "        # presentation_spike_times = unit_spike_times[start_i:end_i]\n",
    "\n",
    "        # Calculate the response rate for each stimulus presentation\n",
    "        responses_by_unit.append((end_is - start_is) / window)\n",
    "\n",
    "    # responses_by_unit has each row a unit, and each column a stimulus, flip so that rows are stimuli\n",
    "    responses = np.transpose(responses_by_unit)\n",
    "\n",
    "    # Extract the labels that match the responses from our sorted stimulus presentations table\n",
    "    labels = np.array(stimulus_presentations['image_name'])\n",
    "    \n",
    "    return responses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4284c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window=.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a10fb5",
   "metadata": {},
   "source": [
    "We will first select a random subset of trials for training the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778764ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_presentations = responses.shape[0]\n",
    "num_train = int(total_presentations * 0.5) # Use 50% of trials for training\n",
    "random_trial_order = np.random.permutation(responses.shape[0])\n",
    "train_indices = random_trial_order[:num_train]\n",
    "\n",
    "training_data = responses[train_indices]\n",
    "training_labels = labels[train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc65166",
   "metadata": {},
   "source": [
    "Next, we'll create the model and fit it to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2735d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(responses[train_indices], labels[train_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7ef93",
   "metadata": {},
   "source": [
    "Now that our model has been trained, we can ask it to classify unlabeled data (i.e., the sets of population firing rates that were not included in our original training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b96247",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = random_trial_order[num_train:]\n",
    "test_data = responses[test_indices]\n",
    "predicted_labels = clf.predict(responses[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260ff5d",
   "metadata": {},
   "source": [
    "We can compare the predicted labels to the actual labels in order to assess the classifier's performance. We'll assess accuracy as the fraction of correctly predicted test images. As a baseline, we'll also compute the accuracy of a uniform random prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = np.unique(labels)\n",
    "\n",
    "actual_labels = labels[test_indices]\n",
    "accuracy = np.mean(actual_labels == predicted_labels)\n",
    "\n",
    "print('Accurary: {}'.format(accuracy))\n",
    "print('Chance level: {}'.format(1/len(conditions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbbe31",
   "metadata": {},
   "source": [
    "We see that we perform better than chance, but not very well! We can get a better sense of classification performance by using the `scikit-learn.model_selection.KFold` iterator to automatically split up the data into \"train\" and \"test\" sets for 5 iterations. Note that the model is fit independently on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b75b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "conditions = np.unique(labels)\n",
    "num_splits = 5\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses):\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(responses[train_indices], labels[train_indices])\n",
    "    \n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses[test_indices])\n",
    "    \n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "    print(accuracy)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c31354",
   "metadata": {},
   "source": [
    "The 5-fold cross-validation roughly agrees with our previous result. Are there particular stimuli that drive the errors? Do assess this we'll look at the confusion matrix, which tells us how frequently stimulus 1 is predicted when any stimulus is shown (and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusions, conditions):\n",
    "    \n",
    "    mean_confusion = np.mean(confusions, axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    im = ax.imshow(mean_confusion, cmap='gray_r')\n",
    "    plt.colorbar(im, ax=ax, label='Fraction of classifications')\n",
    "    \n",
    "    ax.set_xticks(range(len(conditions)), conditions, rotation=45)\n",
    "    ax.set_yticks(range(len(conditions)), conditions)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"Actual label\")\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    \n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a6dbb",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "What structure do you see in the confusion matrix? Do the most accurately decoded images look similar? (Use the session.stimulus_templates object to get the images.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf432d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81d84f81",
   "metadata": {},
   "source": [
    "What do you think would happen if some of the images were new to the mouse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cdd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d56d5f",
   "metadata": {},
   "source": [
    "## Exploring the time course of visual information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a1712",
   "metadata": {},
   "source": [
    "\n",
    "Next we'll examine the time course of information in our population! Or more specifically: how the length of the spike count window affects the decoding accuracy. Can we decode the stimulus perfectly if we integrate spikes for long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa8734",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "First, let's try decoding with a longer response window of .1 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c18775f3",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "How long do we need to integrate spikes in order to decode perfectly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_lengths = np.arange(.01, .1, .02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fc448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de494657",
   "metadata": {},
   "source": [
    "## Relationship between population size and decoding accuracy\n",
    "\n",
    "Next we'll examine how the size of the simultaneously recorded population affects decoding accuracy. In any physiology experiment, we only have a very small window into the overall population response. For example, there are about 500,000 neurons in mouse V1, so in this case we are measuring around 0.02% of the firing rates in this region.\n",
    "\n",
    "As the number of simultaneously recorded neurons increases, we expect that our ability to decode stimulus identity will improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82b8f8",
   "metadata": {},
   "source": [
    "To start with, let's try decoding with a random sample of 10 neurons. Note: here we're using the responses from our longest window above. So, if we used the full population we would be able to decode perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3502d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 10\n",
    "\n",
    "pop_idx = np.random.choice(range(num_units), size=pop_size)\n",
    "responses_pop = responses[:, pop_idx]\n",
    "\n",
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses_pop):\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(responses_pop[train_indices], labels[train_indices])\n",
    "\n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses_pop[test_indices])\n",
    "\n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")\n",
    "\n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b48d81",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "Does the result depend on which 10 neurons we sampled? Let's try another random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c85d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1967ae",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "### Now, let's try to get a sense for how this changes with the number of neurons we use to train the classifier. \n",
    "### How many neurons do you need to decode with roughly 50% accuracy? 80%? 90%? Finish the code below.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sizes = np.arange(1, 50, 5).astype('int')\n",
    "num_resamples = 10\n",
    "\n",
    "accuracies = np.zeros((len(pop_sizes), num_resamples, num_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24c50f",
   "metadata": {},
   "source": [
    "## Exploring correlations between neurons\n",
    "\n",
    "Finally, we turn to examine the structure of the population activity.\n",
    "Does the structure of the population activity matter for this decoding, or is single-neuron tuning the whole name of the game? For example, if neurons 1 and 2 are co-active on trial 1 (both above their individual mean activity), does that carry any extra information? To explore this, we'll look at correlations between their responses.\n",
    "\n",
    "<!-- Based on the plot above, it's clear that neurons are correlated with one another. For example, look at units 35-40 and notice how they tend to have high firing rates or low firing rates on similar trials. -->\n",
    "\n",
    "We'll look at this correlation in much more detail below, but we should first note some assumptions. Primarily, we are studying *spike counts*, or rates within time windows defined by the stimulus. This assumes that all spikes within the windows are equivalent, no matter their relative timing. It also assumes a specific set of time windows (set by the stimulus). In some cases, these assumptions may not be desirable (e.g., in studies of time-lagged spike-spike correlation, frequently used in studies of functional connectivity.)\n",
    "\n",
    "With that tangent aside, let's return to our observation that the neurons' activities (defined here by spike rates) are correlated.\n",
    "\n",
    "<!-- The activities of correlated neural populations have a *lower dimensionality* than the number of neurons. For example, for two perfectly correlated neurons, a single number suffices to describe both of their firing rates. This same idea applies to larger populations, and to less-than-perfect correlations. -->\n",
    "\n",
    "<!-- To explore this property, we will apply the most common dimensionality reduction technique in existence to these data: Principal Component Analysis (PCA). This is a linear dimensionality reduction method (more on this later), and it works by considering the space of all possible neuron responses, wherein each axis of the space is a single neuron's firing rate. PCA finds the directions in this space along which the activities are the most spread out (highest variance) or the least spread out. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc952d9b",
   "metadata": {},
   "source": [
    "## Computing correlation matrices\n",
    "\n",
    "For the following analysis, we will look at Pearson correlations: the Pearson correlation for a pair of neurons is the covariance divided by the product of the neurons' standard deviations. This normalizes the measure so that its maximum is 1 and minimum is -1, which makes it easier to interpret than covariances.\n",
    "\n",
    "So far, we have not considered how much of the covariance or correlation is stimulus-driven (e.g., reflecting neurons with similar tuning responding to the same stimulus at the same time) vs arising from other sources. \n",
    "\n",
    "The correlations due to the stimulus properties are called *signal correlations*, whereas correlations due to other sources (including random variability within the eyes and the brain) are called *noise correlations*. The correlations we considered above encapsulate both of these factors, and are called *total* correlations.\n",
    "\n",
    "To separate these out, we'll now compute and compare all 3 (Pearson) correlation matrices: the total correlations, signal correlations, and noise correlations.\n",
    "\n",
    "First, the total correlations (using `np.corrcoef`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a44fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, let's use a long response window\n",
    "responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72575c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correlations = np.corrcoef(responses.T)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "im = ax.imshow(total_correlations, cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar(im, ax=ax, label='Correlation coefficient')\n",
    "ax.set_title('Total Correlations')\n",
    "ax.set_xlabel('Unit #')\n",
    "ax.set_ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7a26f",
   "metadata": {},
   "source": [
    "Next, we'll compute the signal correlations. These are the correlations in the neurons' average response to each stimulus, computed across stimuli. As the name implies, they tell us how much two neurons' mean (trial averaged) activities co-vary as the stimulus changes.\n",
    "\n",
    "To compute these, we'll first calculate the average activities for each stimulus identify and neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute trial-averaged response to each stimulus (aka tuning curves) using our response array\n",
    "stimuli = stimulus_presentations['image_name'].unique()\n",
    "num_stim = len(stimuli)\n",
    "\n",
    "tuning_curves = np.zeros((num_units, num_stim))\n",
    "\n",
    "for j, stim in enumerate(stimuli):\n",
    "    stim_idx = np.where(labels == stim)\n",
    "    tuning_curves[:, j] = np.mean(responses[stim_idx], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4506570",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(tuning_curves.T);\n",
    "ax.set_xticks(range(len(conditions)), conditions, rotation=45)\n",
    "ax.set_xlabel('Stimulus')\n",
    "ax.set_ylabel('Firing rate')\n",
    "ax.set_title('Tuning curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a334257",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "The signal correlation matrix is the pearson correlation of neuron's trial-averaged responses---the similarity of their tuning curves. Calculate and plot the signal correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da58ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70ebd45c",
   "metadata": {},
   "source": [
    "Finally, let's compute the noise correlations. These are the correlations in the responses to each stimulus, reflecting the (correlated) trial-to-trial variability in the neural population. These correlations can come from synaptic connections (or indirect connections) between the neurons, so that when neuron A fires more on a given trial, neuron B also fires more (excitatory connection), or neuron B fires less (inhibitory connection). The noise correlations can also come from shared input. For example, if neuron C has an excitatory projection to both neurons A and B, then on trials where neuron C has increased firing rate, then both neurons A and B will also show increased firing.\n",
    "\n",
    "These noise correlations are defined on a per-stimulus basis and can vary somewhat between stimuli. For sake of interest, we'll plot below the correlation matrices for two different stimuli, and we'll later make use of the average correlation matrix (averaged over all 8 orientations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95108a22",
   "metadata": {},
   "source": [
    "Since noise correlations are single-trial correlations, if a neuron does not respond to a particular stimulus condition it can generate NaNs when we divide by the standard deviation of the responses. To ignore these, we use numpy's masked array module, numpy.ma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_correlations = np.zeros((len(stimuli), num_units, num_units)) # initialize the noise correlation matrix for each stimulus condition\n",
    "\n",
    "for i, condition in enumerate(stimuli):\n",
    "    condition_idx = np.where(labels == condition)\n",
    "    responses_condition = responses[condition_idx]   \n",
    "    noise_correlations[i] = np.ma.corrcoef(responses_condition.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b88255",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_condition_idx = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "im = ax.imshow(noise_correlations[plot_condition_idx], cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar(im, ax=ax, label='Correlation coefficient')\n",
    "ax.set_title('Noise Correlations, {}'.format(conditions[plot_condition_idx]))\n",
    "ax.set_xlabel('Unit #')\n",
    "ax.set_ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ee0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_condition_idx = -1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "im = ax.imshow(noise_correlations[plot_condition_idx], cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar(im, ax=ax, label='Correlation coefficient')\n",
    "ax.set_title('Noise Correlations, {}'.format(conditions[plot_condition_idx]))\n",
    "ax.set_xlabel('Unit #')\n",
    "ax.set_ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38568c",
   "metadata": {},
   "source": [
    "Note that noise correlations can vary between stimuli! What differences do you see between these two noise correlation matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7e6d2",
   "metadata": {},
   "source": [
    "To get an overall view of the noise correlations, we average them across stimuli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29168721",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_noise_correlations = np.mean(noise_correlations,axis=0)\n",
    "\n",
    "print('Mean noise correlation: {}'.format(np.mean(np.triu(mean_noise_correlations, 1))))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "im = ax.imshow(mean_noise_correlations, cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar(im)\n",
    "ax.set_title('Noise Correlations, mean over stimuli')\n",
    "ax.set_xlabel('Unit #')\n",
    "ax.set_ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cebac",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "A common interpretation of noise correlations is that they can arise from synapses between, or common input to, a pair of neurons. These can also affect (or, through Hebbian learning, reflect) stimulus tuning. So we might expect noise and signal correlations to be correlated.\n",
    "    \n",
    "The scipy.stats function pearsonr computes the pearson correlation and a a p-value from the hypothesis test where the null ypothesis is zero correlation. Are the noise and signal correlations significantly correlated?\n",
    "    \n",
    "Make sure to only compare the off-diagonal elements of the noise and signal correlation matrices, since the diagonal elements are all 1 by definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfe82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda2ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be28c92",
   "metadata": {},
   "source": [
    "Various computational models make predictions about the relation between noise and signal correlations. For example, the local competition algorithm for sparse coding, which was once a leading theory of V1 computation, predicts a negative relationship between noise and signal correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973a18e",
   "metadata": {},
   "source": [
    "## Population decoding with and without noise correlations\n",
    "\n",
    "While the noise correlations are weak, it is worth asking whether or not -- from an information processing standpoint -- we can treat each neuron as independent. In other words, are the noise correlations weak enough that they can be ignored?\n",
    "\n",
    "To test this, we'll return to our decoding analysis, and we will try decoding from synthetic data in which we artificially remove the noise correlations. We do this by trial-shuffling the neural data. This creates a fake dataset in which non-simultaneously-recorded neural activities are assembled to make the population response vectors, and it removes the noise correlations.\n",
    "\n",
    "To do this, we go through the data, and for each stimulus, and for each neuron, we randomly (and independently) re-order the trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bced5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_shuffle_responses(responses, conditions):\n",
    "    \n",
    "    shuffled_responses = responses.copy()\n",
    "\n",
    "    for i, condition in enumerate(conditions):\n",
    "        condition_idx = np.where(labels == condition)\n",
    "\n",
    "        for j in range(num_units):\n",
    "            responses_unit_condition = responses[condition_idx, j].reshape(-1).copy()\n",
    "            np.random.shuffle(responses_unit_condition) # shuffle in place\n",
    "            shuffled_responses[condition_idx, j] = responses_unit_condition\n",
    "            \n",
    "    return shuffled_responses\n",
    "\n",
    "shuffled_responses = trial_shuffle_responses(responses, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17663d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "First, let's double-check that our shuffling worked correctly. Compute the noise correlations in the shuffled data, and plot the histogram of the off-diagonal elements of the true noise correlations and of the shuffled-response noise correlations. \n",
    "    \n",
    "We'd expect to see that the trial-shuffled noise correlations are distributed closely around 0, with non-zero values only due to the finite sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6946785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c06e353",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "Now let's decode from the trial-shuffled responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed54eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de54b31",
   "metadata": {},
   "source": [
    "# With these analyses in hand, we leave you with some questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63c00a",
   "metadata": {},
   "source": [
    "### If you integrate spikes in a fixed window length, how does the decoding accuracy depend on the time since the image presentation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480cbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8cb5af7",
   "metadata": {},
   "source": [
    "### Do noise correlations impact the decoding on specific timescales or for specific population sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59dd27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "167e64c4",
   "metadata": {},
   "source": [
    "### Are the accuracy curves different for familiar vs novel images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce64c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e511b6cf",
   "metadata": {},
   "source": [
    "### Are the accuracy curves different in active vs passive blocks? Do noise correlations have differential impacts in those blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0b507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "278570a3",
   "metadata": {},
   "source": [
    "### Is the structure of the population code different on hit vs miss trials? Can you predict hit vs miss by comparing the lick time to the decoding time course?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269244a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a628ae2",
   "metadata": {},
   "source": [
    "### Are other variables, including behavioral variables, also encoded in the population activity? Can you decode the running speed, pupil diameter, or licking behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78c801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d885d65d",
   "metadata": {},
   "source": [
    "### What about in a different brain area? For example, is the image encoded in CA1 activity? What about in the joint activity across brain areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c273d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
